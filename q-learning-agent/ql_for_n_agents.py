# -*- coding: utf-8 -*-
"""QL_for_N_agents.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RR-_D4UzcFqFavOz3KMOzGkdxKDoyFOu

How to use this file:

This file implements the QLearning for N drones in a predefined grid space.

For your use case, you need to:

- Change the NUMBER_ACTION_COLUMNS properly for your case (e.g for actions 0,...24, the value must be 25. For 0,...124, the value must be 125, and so on)
- Set the hyperparameters properly inside Map \_init\_() class.
- Change any other values in order to customize the process, e.g number of episodes and steps.
"""

import pandas as pd
import numpy as np
import random
import ast
from itertools import combinations
from IPython.display import clear_output
import matplotlib.pyplot as plt

d = pd.read_csv('../pre-datasets/results/reward_2.dat', sep=' ')

# the action columns (0, 1 - 24) are in string mode. Below, we change it to int mode, to make future manipulations easy.
d = d.rename({col: int(col) for col in d.columns if d[col].dtype == 'object'}, axis='columns')

NUMBER_ACTION_COLUMNS = 25  # total number of possible actions

# the table's body are in string mode (e.g. '[945,0.95379]'). Turning it into lists to make future manipulations easy.
for col in range(NUMBER_ACTION_COLUMNS):  # from 0 to 24 (for 2 agents and 5 possible actions each)
    d[col] = d[col].apply(ast.literal_eval)


class Map:
    def __init__(self,
                 dim_grid=10,  # means the grid is 10x10
                 actions_per_agent=5,  # each agent is capable of up,right,down,left and stopped movements
                 agents=2,  # total number of agents in the grid
                 state=0,  # initial state, starts at state 0 (means there is a first position for all agents)
                 alpha=0.2,  # Q-learning algorithm learning rate
                 gamma=0.9,
                 # gamma is the discount factor. It is multiplied by the estimation of the optimal future value.
                 epsilon=1,
                 # epsilon handles the exploration/exploitation trade-off (e.g. epsilon < 0.4 means 40% exploration and 60% exploitation)
                 epsilon_min=0.5,
                 # minimum allowed epsilon. Epsilon will change (reduce) with decay_epsilon function. At begginings, it means more exploration than exploitation.
                 epsilon_decay=0.999
                 # epsilon will decay at each step. For 1000 steps and decay 0.999, for example, epsilon will decay a factor by 0.367 of it initial value.
                 ):
        self.dim_grid = dim_grid
        self.actions_per_agent = actions_per_agent
        self.agents = agents
        self.state = state
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_max = epsilon
        self.epsilon_decay = epsilon_decay
        self.initQTable()

    def initQTable(self):
        # for a grid 10x10 and 2 agents, for example, total stated are C(10x10,2) = 4950 states
        # for 2 agents with 5 movements each, we have 5^2 = 25 possible actions
        self.states = len(list(combinations([i for i in range(self.dim_grid * self.dim_grid)], self.agents)))
        self.qtable = np.zeros(
            (self.states, self.actions_per_agent ** self.agents))  # for grid 10 and 2 agents -> Q(4950,25)

    # gives the next state given the current state for a given action
    def next_state(self, current_state, action):
        return d.loc[current_state, action][0]

    # gives the current qos for a current state
    def current_qos(self, current_state):
        return d.loc[current_state, 'qos']

    # gives the qos of the next state, given the current state and a given action
    def next_qos(self, current_state, action):
        return d.loc[current_state, action][1]

    # epsilon will return to it's initial value for each episode
    def resetEpsilon(self):
        self.epsilon = self.epsilon_max

    # attribute a new value to epsilon after a decay
    def decay_epsilon(self):
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)

    # if current qos is less than the next qos, we have a reward. Otherwise, we have a penalty.
    def reward(self, current_state, action):
        rwd = self.next_qos(current_state, action)
        return rwd

    # given a state, we search over all possible actions that gives the maximum of qtable[state]. Returns the maximum q-value.
    def getMaxQ(self, state):
        return max(self.qtable[state])

    # returns a random choice over all possible actions (0,1,2,...,24) for 2 agents and 5 possible movements, for example.
    def randomAction(self):
        actions = [i for i in range(self.actions_per_agent ** self.agents)]
        return random.choice(actions)

    # given the qtable, gets the action that better results in the biggest q-value.
    def bestAction(self, state):
        movement = np.argmax(self.qtable[state])
        return movement

    # given a state, times it'll be a random action, times it'll be the best action. It depends on epsilon value.
    def getAction(self, state):
        self.decay_epsilon()
        if random.random() < self.epsilon:
            return self.randomAction()
        else:
            return self.bestAction(state)

    # given a state and a given action, it updates the qtable and returns the new state and the reward for the given movement in that state.
    def update(self, state, action):
        newstate = self.next_state(state, action)
        reward = self.reward(state, action)
        self.qtable[state][action] = (1 - self.alpha) * self.qtable[state][action] + self.alpha * (
                    reward + self.gamma * self.getMaxQ(newstate))
        return newstate, reward


rewards = [0]
k = 0

improvements = []

episodes = 300
steps = 1000
m = Map()

episodes_visited_states = {}

for ep in range(episodes):

    state = random.choice(d['state'])
    visited_states = [state]

    start_qos_episode = m.current_qos(state)
    print(f'Qos start state (episode {ep}): {start_qos_episode}')
    m.resetEpsilon()

    for step in range(steps):
        action = m.getAction(state)
        newstate, reward = m.update(state, action)
        state = newstate

        visited_states.append(state)

        mean_reward = ((k + 1) * rewards[-1] + reward) / (k + 2)
        k = k + 1

    episodes_visited_states[ep] = visited_states

    final_qos_episode = m.current_qos(state)
    print(f'Qos final state (episode {ep}): {final_qos_episode}')

    if final_qos_episode > start_qos_episode:
        print(True)
        improvements.append(1)
    else:
        print(False)
        improvements.append(0)

    print('---------' * 5)

    # Média móvel da recompensa no término de cada episodio
    rewards.append(mean_reward)
    # clear_output(wait=True)
    print(f"episode: {ep:0{5}}/{episodes} - R: {mean_reward:.{8}f}")
print(
    f'For {episodes} episodes, there was {sum(improvements)} improvements ({round(sum(improvements) * 100 / episodes, 2)}%) and {episodes - sum(improvements)} worse results ({round((episodes - sum(improvements)) * 100 / episodes, 2)}%)')


# following only the policy
def follow_policy(map_obj, state):
    action = map_obj.bestAction(state)
    return action


for n in range(5):
    state = random.choice(d['state'])
    print(f'start-->{state}')
    for step in range(20):
        action = follow_policy(m, state)
        state = m.next_state(state, action)
        print(state)

###############
fig, ax = plt.subplots(figsize=(6, 3))
ax.plot(np.arange(len(rewards)), rewards, linestyle='solid', color='blue', linewidth=2)
ax.set_title(f'Agents: {m.agents} , Grid: {m.dim_grid}x{m.dim_grid}, Movements per Agent: {m.actions_per_agent}')
ax.set_xlabel("Episodes")
ax.set_ylabel("Avg Reward")
plt.show()

lsts = []
for v in episodes_visited_states.values():
    lsts.extend(v)
lsts

plt.hist(lsts, bins=100);

d['qos'].sort_values(ascending=False)
